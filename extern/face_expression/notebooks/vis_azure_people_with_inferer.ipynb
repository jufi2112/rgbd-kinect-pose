{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "import face_expression\n",
    "from face_expression import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/Vol0/user/k.iskakov/dev/pykinect\")\n",
    "from pose_dataset_example import *\n",
    "\n",
    "# # renat\n",
    "# sys.path.append(\"/home/renat/workdir/git/kavatar_realtime/extern/pykinect/\")\n",
    "# from pose_dataset_example import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "op_root = \"/Vol1/dbstore/datasets/k.iskakov/azure_people_test/openpose_vakhitov_format\"\n",
    "smplx_root = \"/Vol1/dbstore/datasets/a.vakhitov/kinect_dataset/full_testcap_30\"\n",
    "\n",
    "# setup dirs for result\n",
    "result_root_dir = \"/Vol1/dbstore/datasets/k.iskakov/projects/face_expression\"\n",
    "result_dir = os.path.join(result_root_dir, \"artifacts\", \"azure_people_video_inferer\")\n",
    "\n",
    "frame_dir = os.path.join(result_dir, \"frames\")\n",
    "output_video_path = os.path.join(result_dir, \"video.mp4\")\n",
    "\n",
    "shutil.rmtree(output_video_path, ignore_errors=True)\n",
    "shutil.rmtree(frame_dir, ignore_errors=True)\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "# # input renat\n",
    "# op_root = \"/home/renat/workdir/data/kavatar/face_expression/openpose_vakhitov_format\"\n",
    "# smplx_root = \"/home/renat/workdir/data/kavatar/face_expression/full_testcap_30\"\n",
    "\n",
    "# # setup dirs for result\n",
    "# result_root_dir = \"/home/renat/workdir/data/kavatar/face_expression/result\"\n",
    "# result_dir = os.path.join(result_root_dir, \"artifacts\", \"azure_people_video_inferer\")\n",
    "\n",
    "# frame_dir = os.path.join(result_dir, \"frames\")\n",
    "# output_video_path = os.path.join(result_dir, \"video.mp4\")\n",
    "\n",
    "# shutil.rmtree(output_video_path, ignore_errors=True)\n",
    "# shutil.rmtree(frame_dir, ignore_errors=True)\n",
    "# os.makedirs(frame_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis\n",
    "def load_joints_poses_dict(root_folder_bt, pid_lbl, dev_lbl):\n",
    "    joints_json_path = root_folder_bt + '/' + pid_lbl + '/bt.json'\n",
    "    with open(joints_json_path) as json_file:\n",
    "        joints_poses_list = json.load(json_file)\n",
    "\n",
    "    joints_poses_dict = dict()\n",
    "    for i in tqdm(range(len(joints_poses_list))):\n",
    "        try:\n",
    "            joints_poses, joints_mask = get_kinect_joints(joints_poses_list, i, dev_lbl)\n",
    "            joints_poses_dict[joints_poses_list[i]['frame_index']] = joints_poses\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            \n",
    "    return joints_poses_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder_bt = \"/Vol0/user/r.bashirov/workdir/git/data/smplx_kinect/test_capture/offline_processor2\"\n",
    "root_folder_rgb = \"/Vol0/user/r.bashirov/workdir/git/data/smplx_kinect/test_capture/offline_processor2\"\n",
    "\n",
    "pk_root = \"/Vol1/dbstore/datasets/a.vakhitov/projects/pykinect_fresh/pykinect/data/fit_realtime_data\"\n",
    "vposer_ckpt = \"/Vol1/dbstore/datasets/a.vakhitov/projects/pykinect_fresh/smplify-x/smplify-x-data/vposer_v1_0\"\n",
    "\n",
    "## renat\n",
    "# root_folder_bt = \"/home/renat/workdir/data/kavatar/face_expression/offline_processor2\"\n",
    "# root_folder_rgb = root_folder_bt\n",
    "\n",
    "# # pk_root = \"/Vol1/dbstore/datasets/a.vakhitov/projects/pykinect_fresh/pykinect/data/fit_realtime_data\"\n",
    "# # pk_root = \"/mnt/hdd10/kavatar/release_data/2020_09_09/pykinect\"\n",
    "# pk_root = \"/home/renat/workdir/data/kavatar/face_expression/pk_root\"\n",
    "# vposer_ckpt = \"/home/renat/workdir/data/kavatar/face_expression/vposer_v1_0\"\n",
    "\n",
    "fit_type = 'joints_op_face_hand'\n",
    "\n",
    "start_index, n_frames, step = 10, 700, 2\n",
    "frame_ids = np.arange(start_index, start_index + n_frames, step)\n",
    "\n",
    "fit_dataset_path = smplx_root\n",
    "\n",
    "dev_lbl = '000062692912'\n",
    "pid_lbl = '04_simple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam_params_decode(cam_params):\n",
    "    K_d_rgb = np.eye(3)\n",
    "    K_d_rgb[0, 0] = cam_params['rgb_intrinsics']['fx']\n",
    "    K_d_rgb[1, 1] = cam_params['rgb_intrinsics']['fy']\n",
    "    K_d_rgb[0, 2] = cam_params['rgb_intrinsics']['cx']\n",
    "    K_d_rgb[1, 2] = cam_params['rgb_intrinsics']['cy']\n",
    "\n",
    "    K_rgb = np.eye(3)\n",
    "    K_rgb[0, 0] = cam_params['rgb_undistorted_intrinsics']['fx']\n",
    "    K_rgb[1, 1] = cam_params['rgb_undistorted_intrinsics']['fy']\n",
    "    K_rgb[0, 2] = cam_params['rgb_undistorted_intrinsics']['cx']\n",
    "    K_rgb[1, 2] = cam_params['rgb_undistorted_intrinsics']['cy']\n",
    "\n",
    "    d_rgb = np.zeros(8)\n",
    "    d_rgb[0] = cam_params['rgb_intrinsics']['k1']\n",
    "    d_rgb[1] = cam_params['rgb_intrinsics']['k2']\n",
    "    d_rgb[2] = cam_params['rgb_intrinsics']['p1']\n",
    "    d_rgb[3] = cam_params['rgb_intrinsics']['p2']\n",
    "    d_rgb[4] = cam_params['rgb_intrinsics']['k3']\n",
    "    d_rgb[5] = cam_params['rgb_intrinsics']['k4']\n",
    "    d_rgb[6] = cam_params['rgb_intrinsics']['k5']\n",
    "    d_rgb[7] = cam_params['rgb_intrinsics']['k6']\n",
    "\n",
    "    T_depth_to_rgb = np.eye(4)\n",
    "    rvec = np.zeros(3)\n",
    "    tvec = np.zeros(3)\n",
    "    for i in range(0, 3):\n",
    "        rvec[i] = cam_params['depth_to_rgb']['r'][i]\n",
    "        tvec[i] = cam_params['depth_to_rgb']['t'][i]\n",
    "\n",
    "    tvec /= 1000\n",
    "\n",
    "    R, jac = cv2.Rodrigues(rvec)\n",
    "    T_depth_to_rgb[0:3, 0:3] = R\n",
    "    T_depth_to_rgb[0:3, 3] = tvec\n",
    "\n",
    "    return K_d_rgb, d_rgb, K_rgb, T_depth_to_rgb\n",
    "\n",
    "# load device calibs\n",
    "mkv_meta_json_path = root_folder_bt + '/' + pid_lbl + '/' + dev_lbl + '/mkv_meta.json'\n",
    "with open(mkv_meta_json_path) as f:\n",
    "    cam_params = json.load(f)\n",
    "\n",
    "K_d_rgb, d_rgb, K_rgb, T_depth_to_rgb = cam_params_decode(cam_params)\n",
    "K_d_rgb = K_d_rgb\n",
    "d_rgb = d_rgb\n",
    "rvec, jac = cv2.Rodrigues(T_depth_to_rgb[0:3, 0:3])\n",
    "tvec = T_depth_to_rgb[0:3, 3]\n",
    "rvec = rvec\n",
    "tvec = tvec\n",
    "\n",
    "# if not os.path.exists(mkv_meta_json_path):\n",
    "#     print('dev not found')\n",
    "# K_rgb, K_d, T_depth_to_rgb = read_device_calibs(mkv_meta_json_path)\n",
    "\n",
    "joints_poses_dict = load_joints_poses_dict(root_folder_bt, pid_lbl, dev_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load inferer\n",
    "# import lib\n",
    "# FACE_EXPRESSION_SRC_ROOT = \"/home/renat/Desktop/face_expression\"\n",
    "# sys.path.append(FACE_EXPRESSION_SRC_ROOT)\n",
    "from face_expression import Inferer, Cropper\n",
    "\n",
    "# setup device and checkpoint\n",
    "device = 'cuda:0'\n",
    "\n",
    "checkpoint_path = \"/Vol1/dbstore/datasets/k.iskakov/share/face_expression/gold_checkpoints/siamese+mouth+keypoints_3d_loss+expression_loss/checkpoint_000019.pth\"\n",
    "config_path = \"/Vol1/dbstore/datasets/k.iskakov/share/face_expression/gold_checkpoints/siamese+mouth+keypoints_3d_loss+expression_loss/config.yaml\"\n",
    "\n",
    "inferer = Inferer(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    device='cuda:0'\n",
    ")\n",
    "    \n",
    "cropper = Cropper(\n",
    "    rvec,\n",
    "    tvec,\n",
    "    K_d_rgb,\n",
    "    dist=d_rgb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "times = []\n",
    "for fid in tqdm(frame_ids):\n",
    "    if fid not in joints_poses_dict:\n",
    "        continue\n",
    "        \n",
    "    joints_poses = joints_poses_dict[fid]\n",
    "\n",
    "    # load rgb image   \n",
    "    img_path = root_folder_rgb + '/' + pid_lbl + '/' + dev_lbl + '/color_undistorted/' + str(fid).zfill(6) + '.jpg'\n",
    "    img = cv2.imread(img_path)[:, :, ::-1]\n",
    "\n",
    "    im_size = (img.shape[1], img.shape[0])\n",
    "\n",
    "    if img is None:\n",
    "        print('no rgb frame')\n",
    "    \n",
    "    ### (!) prediction\n",
    "#     for i in range(1000):\n",
    "    start_time = time.time()\n",
    "    img_cropped, face_bbox = cropper.forward(img, joints_poses)\n",
    "    expression_pred, jaw_pose_pred, keypoints_2d = inferer.forward(img_cropped, beta=None)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    times.append(elapsed_time)\n",
    "    mean_time = np.mean(times)\n",
    "    print(f\"Mean time/FPS: {mean_time:.3f}/{1/mean_time:.1f}\")\n",
    "    \n",
    "    # infer SMPLX\n",
    "    part_id = -1\n",
    "    rot_t, trans_t, pose_body_t, pose_hand_t, pose_jaw_t, pose_eye_t, betas_t, expr_t, is_male, is_vposer = \\\n",
    "        get_smplx_params(fit_dataset_path, dev_lbl, fit_type, pid_lbl, part_id, fid, vposer_ckpt)\n",
    "    \n",
    "    exp_bm = init_smplx_inf_model(pk_root, is_male, fit_type)\n",
    "    \n",
    "    faces_np = exp_bm.f.detach().cpu().numpy()\n",
    "    main_renderer = init_renderer(faces_np)\n",
    "    \n",
    "    # GT\n",
    "    body = exp_bm(rot_t, pose_body_t, pose_hand_t, pose_jaw_t, pose_eye_t, betas_t, trans_t, expr_t)\n",
    "    verts = body.v[0].detach().cpu().numpy()\n",
    "\n",
    "    main_renderer.render(verts, np.eye(3), K=K_rgb)\n",
    "    gt_cimg = main_renderer.get_aligned_camera_image()[:, :, :3]\n",
    "    \n",
    "    # pred\n",
    "    expression_pred_t = torch.from_numpy(expression_pred).unsqueeze(0)\n",
    "    jaw_pose_pred_t = torch.from_numpy(jaw_pose_pred).unsqueeze(0)\n",
    "    \n",
    "    body_pred = exp_bm(rot_t, pose_body_t, pose_hand_t, jaw_pose_pred_t, pose_eye_t, betas_t, trans_t, expression_pred_t)\n",
    "    verts_pred = body_pred.v[0].detach().cpu().numpy()\n",
    "\n",
    "    main_renderer.render(verts_pred, np.eye(3), K=K_rgb)\n",
    "    pred_cimg = main_renderer.get_aligned_camera_image()[:, :, :3]\n",
    "    \n",
    "    # crop\n",
    "    img_croppped = utils.common.crop_image(img, face_bbox)\n",
    "    keypoints_2d -= face_bbox[:2]\n",
    "    img_croppped = utils.vis.vis_image_with_keypoints_2d_numpy(keypoints_2d, img_croppped, 1)\n",
    "    \n",
    "    cimgs_cropped = [utils.common.crop_image(x, face_bbox) for x in (pred_cimg, gt_cimg)]\n",
    "\n",
    "    cobmibned_img = np.concatenate([img_croppped, cimgs_cropped[0], cimgs_cropped[1]], axis=1)\n",
    "    if count % 50 == 0:\n",
    "        plt.imshow(cobmibned_img)\n",
    "        plt.show()\n",
    "\n",
    "    img_path = os.path.join(frame_dir, f\"{count:06d}.jpg\")\n",
    "    cv2.imwrite(img_path, cobmibned_img[:, :, ::-1])\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    \"ffmpeg\",\n",
    "    \"-y\",\n",
    "    \"-framerate\", \"25\",\n",
    "    \"-i\", os.path.join(frame_dir, \"%06d.jpg\"),\n",
    "    \"-c:v\", \"libx264\",\n",
    "    \"-vf\", \"fps=25\",\n",
    "    \"-vf\", \"pad=ceil(iw/2)*2:ceil(ih/2)*2\",\n",
    "    \"-pix_fmt\", \"yuv420p\",\n",
    "    output_video_path\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "if result.returncode:\n",
    "    raise ValueError(result.stderr.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Output video path:\\n{output_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
