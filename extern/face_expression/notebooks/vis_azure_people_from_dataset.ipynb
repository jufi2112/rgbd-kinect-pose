{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "from omegaconf import OmegaConf\n",
    "import pydoc\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import utils.common, utils.vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copytree(src, dst, symlinks=False, ignore=None):\n",
    "    for item in os.listdir(src):\n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "        else:\n",
    "            shutil.copy2(s, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/Vol1/dbstore/datasets/k.iskakov/projects/face_expression\"\n",
    "\n",
    "# run_name = \"run-20200910_120005-2icqhqix\" # siamese_mediapipe_2d\n",
    "# run_name = \"run-20200910_142958-27s0s7pi\" # siamese_dropout\n",
    "# run_name = \"run-20200911_003858-1zs8eycn\" # siamese_jaw_pose.weight-5.0\n",
    "# run_name = \"run-20200911_005939-vwlb8xjz\" # siamese_small\n",
    "\n",
    "# run_name = \"run-20200915_200519-nitmitct\" # siamese_bbox_filter\n",
    "\n",
    "# run_name = \"run-20200916_182119-3khcgm0a\"  # siamese_normalize_area-False_jaw_pose_weight-10.0\n",
    "# run_name = \"run-20200916_212159-2kfaqcv4\" # siamese_keypoint_l2_normalize_area-False\n",
    "# run_name = \"run-20200917_122110-1qk66uzu\" # siamese+keypoint_l2_loss+normalize-image_shape\n",
    "\n",
    "# run_name = \"run-20200917_181208-g15oyjuo\" # siamese+mediapipe_normalization\n",
    "# run_name = \"run-20200917_181214-2p2mcq7o\" # siamese+mediapipe_normalization+expression_weight-10\n",
    "# run_name = \"run-20200917_181220-23sm1rck\" # siamese+mediapipe_normalization+use_beta-false\n",
    "\n",
    "# run_name = \"run-20200923_190202-3lf0gggu\"  # siamese+keypoints_3d\n",
    "# run_name = \"run-20200923_185641-256g37gk\"  # siamese+mouth\n",
    "run_name = \"run-20200923_180309-2vciol9p\"  # siamese+keypoints_3d_loss+expression_loss\n",
    "# run_name = \"run-20200923_180225-3kupdul7\"  # siamese+keypoints_3d_loss\n",
    "\n",
    "experiment_dir = os.path.join(project_dir, \"wandb\", run_name)\n",
    "\n",
    "# checkpoint\n",
    "checkpoint_path = utils.common.get_lastest_checkpoint(os.path.join(experiment_dir, \"checkpoints\"))\n",
    "checkpoint_name = os.path.basename(checkpoint_path)\n",
    "print(f\"Checkpoint: {os.path.basename(checkpoint_path)}\")\n",
    "\n",
    "# load config\n",
    "config_path = os.path.join(experiment_dir, \"config.yaml\")\n",
    "with open(config_path) as f:\n",
    "    config = OmegaConf.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner\n",
    "runner_cls = pydoc.locate(config.runner.cls)\n",
    "runner = runner_cls(config)\n",
    "runner = runner.to(config.device)\n",
    "\n",
    "state_dict = torch.load(checkpoint_path)\n",
    "runner.load_state_dict(state_dict)\n",
    "\n",
    "runner.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import get_dataloaders\n",
    "\n",
    "# load azure_people_test dataset\n",
    "azure_people_test_data_config_path = \"../config/data/azure_people_test.yaml\"\n",
    "with open(azure_people_test_data_config_path) as f:\n",
    "    azure_people_test_data_config = OmegaConf.load(f)\n",
    "    \n",
    "config.data.test = azure_people_test_data_config.data.test\n",
    "\n",
    "modes = ('test',)\n",
    "for mode in modes:\n",
    "    config.data[mode].dataloader.args.batch_size = 128\n",
    "    config.data[mode].dataloader.args.num_workers = 8\n",
    "    config.data[mode].dataloader.args.shuffle = False\n",
    "    \n",
    "    \n",
    "#     config.data[mode].dataset.args.sample_range = [0, float('+inf'), 5]\n",
    "\n",
    "dataloaders = get_dataloaders(config, splits=modes)\n",
    "dataloader = dataloaders['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dirs for result\n",
    "root_result_dir = os.path.join(config.log.project_dir, \"artifacts\", \"azure_people_video\")\n",
    "result_dir = os.path.join(root_result_dir, config.log.experiment_name)\n",
    "\n",
    "frame_dir = os.path.join(result_dir, f\"frames#{config.log.experiment_name}#{checkpoint_name}\")\n",
    "output_video_path = os.path.join(result_dir, f\"video#{config.log.experiment_name}#{checkpoint_name}.mp4\")\n",
    "smplx_dir = os.path.join(result_dir, f\"smplx#{config.log.experiment_name}#{checkpoint_name}\")\n",
    "\n",
    "shutil.rmtree(output_video_path, ignore_errors=True)\n",
    "for d in (frame_dir, smplx_dir):\n",
    "    shutil.rmtree(d, ignore_errors=True)\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "    \n",
    "# copy smplx structure\n",
    "op_root = \"/Vol1/dbstore/datasets/k.iskakov/azure_people_test/openpose_vakhitov_format\"\n",
    "smplx_root = \"/Vol1/dbstore/datasets/a.vakhitov/kinect_dataset/full_testcap_30\"\n",
    "\n",
    "copytree(smplx_root, smplx_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dict will be soon edited and then saved\n",
    "smplx_dict = defaultdict(dict)\n",
    "\n",
    "subject_ids = sorted(os.listdir(op_root))\n",
    "for subject_id in tqdm(subject_ids):\n",
    "    camera_ids = sorted(os.listdir(os.path.join(op_root, subject_id)))\n",
    "    \n",
    "    for camera_id in camera_ids:\n",
    "        expression_path = os.path.join(smplx_dir, subject_id, \"mv\", camera_id, \"joints_op_face_hand\", \"expressions.npy\")\n",
    "        expression = np.load(expression_path)\n",
    "        \n",
    "        pose_path = os.path.join(smplx_dir, subject_id, \"mv\", camera_id, \"joints_op_face_hand\", \"poses.npy\")\n",
    "        pose = np.load(pose_path)\n",
    "        \n",
    "        smplx_dict[subject_id][camera_id] = {\n",
    "            'expression': expression,\n",
    "            'pose': pose\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_batches = float('+inf')\n",
    "# n_batches = 1\n",
    "count = 0\n",
    "for i, input_dict in tqdm(enumerate(dataloader), total=min(n_batches, len(dataloader))):\n",
    "    with torch.no_grad():\n",
    "        input_dict = utils.common.dict2device(input_dict, config.device, dtype=torch.float32)\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "        \n",
    "        output_dict = runner.forward(input_dict)\n",
    "        \n",
    "        expression_pred_norm = np.abs(output_dict['expression_pred'].cpu().numpy()).mean()\n",
    "        expression_norm = np.abs(input_dict['expression'].cpu().numpy()).mean()\n",
    "        print(f\"expression norm: {expression_pred_norm}, {expression_norm}\")\n",
    "        \n",
    "        input_dict = utils.common.dict2device(input_dict, 'cpu')\n",
    "        output_dict = utils.common.dict2device(output_dict, 'cpu')\n",
    "\n",
    "        batch_size = input_dict['pose'].shape[0]\n",
    "        for batch_index in range(batch_size):\n",
    "            subject_id, camera_id, _, seq_index = [input_dict['key'][key_index][batch_index] for key_index in range(4)]\n",
    "            print(subject_id, camera_id)\n",
    "            seq_index = seq_index.item()\n",
    "\n",
    "            jaw_pose_pred = output_dict['jaw_pose_pred'][batch_index].detach().cpu().numpy()\n",
    "            expression_pred = output_dict['expression_pred'][batch_index].detach().cpu().numpy()\n",
    "            \n",
    "            smplx_dict[subject_id][camera_id]['pose'][seq_index, 32:35] = jaw_pose_pred\n",
    "            smplx_dict[subject_id][camera_id]['expression'][seq_index] = expression_pred / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save edited dict\n",
    "subject_ids = sorted(os.listdir(op_root))\n",
    "for subject_id in tqdm(subject_ids):\n",
    "    camera_ids = sorted(os.listdir(os.path.join(op_root, subject_id)))\n",
    "    \n",
    "    for camera_id in camera_ids:\n",
    "        expression_path = os.path.join(smplx_dir, subject_id, \"mv\", camera_id, \"joints_op_face_hand\", \"expressions_pred.npy\")\n",
    "        np.save(expression_path, smplx_dict[subject_id][camera_id]['expression'])\n",
    "        \n",
    "        pose_path = os.path.join(smplx_dir, subject_id, \"mv\", camera_id, \"joints_op_face_hand\", \"poses_pred.npy\")\n",
    "        np.save(pose_path, smplx_dict[subject_id][camera_id]['pose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis (iz govna i palok)\n",
    "sys.path.append(\"/Vol0/user/k.iskakov/dev/pykinect\")\n",
    "from pose_dataset_example import *\n",
    "\n",
    "KINECT_NOSE_INDEX = 27\n",
    "KINECT_EYE_LEFT_INDEX = 28\n",
    "KINECT_EAR_LEFT_INDEX = 29\n",
    "KINECT_EYE_RIGHT_INDEX = 30\n",
    "KINECT_EAR_RIGHT_INDEX = 31\n",
    "\n",
    "KINECT_FACE_INDICES = [\n",
    "    KINECT_NOSE_INDEX, \n",
    "    KINECT_EYE_LEFT_INDEX, \n",
    "    KINECT_EAR_LEFT_INDEX, \n",
    "    KINECT_EYE_RIGHT_INDEX,\n",
    "    KINECT_EAR_RIGHT_INDEX\n",
    "]\n",
    "\n",
    "def project_kinect_joints(kinect_joints, P_cw, im_size):\n",
    "    n_j = kinect_joints.shape[0]\n",
    "    j_h = np.concatenate([kinect_joints, np.ones((n_j, 1))], axis=1)\n",
    "    j_cam_proj = j_h @ P_cw.T\n",
    "    j_scr = j_cam_proj[:, 0:2] / np.tile(j_cam_proj[:, 2].reshape(-1, 1), (1, 2))\n",
    "    j_scr_i = (j_scr + 0.5).astype(int)\n",
    "    j_scr_i[:, 0] = np.clip(j_scr_i[:, 0], 0, im_size[0]-1)\n",
    "    j_scr_i[:, 1] = np.clip(j_scr_i[:, 1], 0, im_size[1]-1)\n",
    "    return j_scr_i\n",
    "\n",
    "def crop_bbox_by_keypoints_2d(keypoints_2d):\n",
    "    x_min, y_min = np.min(keypoints_2d, axis=0)\n",
    "    x_max, y_max = np.max(keypoints_2d, axis=0)\n",
    "\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "\n",
    "    # size = max(width, height)\n",
    "    bbox = x_min, y_min, x_min + width, y_min + height\n",
    "    bbox = utils.common.get_square_bbox(bbox)\n",
    "\n",
    "    bbox = utils.common.scale_bbox(bbox, 3)\n",
    "    \n",
    "    return bbox \n",
    "\n",
    "def load_joints_poses_dict(root_folder_bt, pid_lbl, dev_lbl):\n",
    "    joints_json_path = root_folder_bt + '/' + pid_lbl + '/bt.json'\n",
    "    with open(joints_json_path) as json_file:\n",
    "        joints_poses_list = json.load(json_file)\n",
    "\n",
    "    joints_poses_dict = dict()\n",
    "    for i in tqdm(range(len(joints_poses_list))):\n",
    "        try:\n",
    "            joints_poses, joints_mask = get_kinect_joints(joints_poses_list, i, dev_lbl)\n",
    "            joints_poses_dict[joints_poses_list[i]['frame_index']] = joints_poses\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load joints. Reason: {e}\")\n",
    "            \n",
    "    return joints_poses_dict\n",
    "\n",
    "\n",
    "def get_smplx_params(dataset_path, dev_lbl, fit_type, pid_lbl, part_id, fid, vposer_ckpt, prefix=''):\n",
    "    if part_id >= 0:\n",
    "        part_path = dataset_path + '/' + pid_lbl + '/mv/'+ dev_lbl + '/' + fit_type + '/part_' + str(part_id) + '/'                         \n",
    "    else:\n",
    "        part_path = dataset_path + '/' + pid_lbl + '/mv/'+ dev_lbl + '/' + fit_type + '/'                         \n",
    "    poses = np.load(part_path + f'/poses{prefix}.npy')\n",
    "    face_expressions = np.load(part_path + f'/expressions{prefix}.npy')\n",
    "    betas = np.load(part_path + '/betas.npy')    \n",
    "    fid_lst = np.load(part_path + '/fid_lst.npy')\n",
    "    with open(part_path + '/config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    #do we use vposer embeddings\n",
    "    is_vposer = config['is_vposer']\n",
    "    #gender of a subject\n",
    "    is_male = config['is_male']    \n",
    "    #id of a device (used to decode the rigid pose of the device)\n",
    "    \n",
    "    #load the device pose\n",
    "    dev_lst = config['dev_lst']\n",
    "    dev_id = 0\n",
    "    while dev_lst[dev_id] != dev_lbl:\n",
    "        dev_id += 1        \n",
    "    dev_orient = None\n",
    "    dev_trans = None\n",
    "    if dev_id > 0:\n",
    "        dev_orient = np.load(part_path + '/dev_orient.npy')\n",
    "        dev_trans = np.load(part_path + '/dev_trans.npy')\n",
    "        \n",
    "    fid_id = np.nonzero(fid_lst == int(fid))[0][0]\n",
    "    pose = poses[fid_id]\n",
    "    expression = face_expressions[fid_id]\n",
    "        \n",
    "    rot = pose[-3:]\n",
    "    trans = pose[-6:-3]    \n",
    "    if is_vposer:        \n",
    "        from human_body_prior.tools.model_loader import load_vposer\n",
    "        #load the vposer model        \n",
    "        vposer, _ = load_vposer(vposer_ckpt, vp_model='snapshot')\n",
    "        vposer.eval()                \n",
    "        pose_body_vp = torch.tensor(pose[0:32]).reshape(1, -1)        \n",
    "        #convert from vposer to rotation matrices\n",
    "        pose_body_mats = vposer.decode(pose_body_vp).reshape(-1, 3, 3).detach().cpu().numpy()\n",
    "        pose_body = np.zeros(63)\n",
    "        for i in range(0, pose_body_mats.shape[0]):\n",
    "            rot_vec, jac = cv2.Rodrigues(pose_body_mats[i])\n",
    "            pose_body[3*i : 3*i+3] = rot_vec.reshape(-1)        \n",
    "        pose_jaw = pose[32:35]\n",
    "        pose_eye = pose[35:41]\n",
    "        pose_hand = pose[41:-6]        \n",
    "    else:\n",
    "        pose_body = pose[0:63]\n",
    "        pose_jaw = pose[63:66]\n",
    "        pose_eye = pose[66:72]\n",
    "        pose_hand = pose[72:-6]        \n",
    "    \n",
    "    if dev_orient is not None:\n",
    "        rot_mat, jac = cv2.Rodrigues(rot.reshape(3, 1))\n",
    "        dev_mat, jac = cv2.Rodrigues(dev_orient.reshape(3, 1))\n",
    "        rot_mat = dev_mat @ rot_mat \n",
    "        rot, jac = cv2.Rodrigues(rot_mat)\n",
    "        trans = dev_mat @ trans.reshape(3, 1) + dev_trans.reshape(3, 1)    \n",
    "    rot_t = to_tensor(rot)\n",
    "    trans_t = to_tensor(trans)\n",
    "    pose_body_t = to_tensor(pose_body)\n",
    "    pose_hand_t = to_tensor(pose_hand)\n",
    "    pose_jaw_t = to_tensor(pose_jaw)\n",
    "    pose_eye_t = to_tensor(pose_eye)\n",
    "    betas_t = to_tensor(betas)\n",
    "    expr_t = to_tensor(expression * 1e2)\n",
    "    return rot_t, trans_t, pose_body_t, pose_hand_t, pose_jaw_t, pose_eye_t, betas_t, expr_t, is_male, is_vposer\n",
    "\n",
    "def init_smplx_inf_model(pk_root, is_male, fit_type, neutral=False):\n",
    "    #set gender-dependent model paths and load model parameters\n",
    "    if is_male:\n",
    "        s2v = np.load(pk_root + '/rob75_val/s2k_m.npy')        \n",
    "        bm_path = pk_root + '/body_models_1/smplx/SMPLX_MALE.npz'\n",
    "    else:\n",
    "        s2v = np.load(pk_root + '/rob75_val/s2k_f.npy')        \n",
    "        bm_path = pk_root + '/body_models_1/smplx/SMPLX_FEMALE.npz'\n",
    "        \n",
    "    if neutral:\n",
    "        s2v = np.load(pk_root + '/rob75_val/s2k_m.npy')\n",
    "        bm_path = pk_root + '/body_models_1/smplx/SMPLX_NEUTRAL.npz'\n",
    "        \n",
    "    w_add = np.load(pk_root + '/rob75_val/weights.npy')\n",
    "    w_add = softmax(w_add, axis=1)        \n",
    "    \n",
    "    #number of the pca components in the hand poses\n",
    "    n_pca = 6\n",
    "    if fit_type == 'joints_op_face_hand':\n",
    "        n_pca = 12\n",
    "        \n",
    "    #init the smplx inference model                \n",
    "    exp_bm = ExpBodyModel(bm_path, is_hand_pca=True, num_pca_comps=n_pca, s2v=s2v, w_add=w_add)\n",
    "    return exp_bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder_bt = '/Vol0/user/r.bashirov/workdir/git/data/smplx_kinect/test_capture/offline_processor2'\n",
    "root_folder_rgb = \"/Vol0/user/r.bashirov/workdir/git/data/smplx_kinect/test_capture/offline_processor2\"\n",
    "\n",
    "pk_root = '/Vol1/dbstore/datasets/a.vakhitov/projects/pykinect_fresh/pykinect/data/fit_realtime_data/'\n",
    "vposer_ckpt = '/Vol1/dbstore/datasets/a.vakhitov/projects/pykinect_fresh/smplify-x/smplify-x-data/vposer_v1_0/'\n",
    "\n",
    "fit_type = 'joints_op_face_hand'\n",
    "\n",
    "start_index, n_frames, step = 10, 700, 2\n",
    "frame_ids = np.arange(start_index, start_index + n_frames, step)\n",
    "\n",
    "fit_dataset_path = smplx_dir\n",
    "\n",
    "dev_lbl = '000062692912'\n",
    "pid_lbl = '04_simple'\n",
    "\n",
    "#load device calibs\n",
    "mkv_meta_json_path = root_folder_bt + '/' + pid_lbl + '/' + dev_lbl + '/mkv_meta.json'\n",
    "if not os.path.exists(mkv_meta_json_path):\n",
    "    print('dev not found')\n",
    "K_rgb, K_d, T_depth_to_rgb = read_device_calibs(mkv_meta_json_path)\n",
    "\n",
    "joints_poses_dict = load_joints_poses_dict(root_folder_bt, pid_lbl, dev_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for fid in tqdm(frame_ids):\n",
    "#     try:\n",
    "        #load kinect skeletons\n",
    "        joints_poses = joints_poses_dict[fid]\n",
    "\n",
    "        #load rgb image   \n",
    "        img_path = root_folder_rgb + '/' + pid_lbl + '/' + dev_lbl + '/color_undistorted/' + str(fid).zfill(6) + '.jpg'\n",
    "        img = cv2.imread(img_path)[:, :, ::-1]\n",
    "\n",
    "        im_size = (img.shape[1], img.shape[0])\n",
    "#         plt.imshow(img[:, :, ::-1])\n",
    "#         plt.show()\n",
    "        if img is None:\n",
    "            print('no rgb frame')\n",
    "        \n",
    "        cimgs = []\n",
    "        for prefix in ('_pred', ''):\n",
    "            #load fitted SMPLX params    \n",
    "            part_id = -1\n",
    "            rot_t, trans_t, pose_body_t, pose_hand_t, pose_jaw_t, pose_eye_t, betas_t, expr_t, is_male, is_vposer = \\\n",
    "                get_smplx_params(fit_dataset_path, dev_lbl, fit_type, pid_lbl, part_id, fid, vposer_ckpt, prefix=prefix)\n",
    "\n",
    "            #initialize the model\n",
    "            exp_bm = init_smplx_inf_model(pk_root, is_male, fit_type, neutral=False)\n",
    "\n",
    "            #Initialize the renderer    \n",
    "            faces_np = exp_bm.f.detach().cpu().numpy()\n",
    "            main_renderer = init_renderer(faces_np)                \n",
    "            kinect_kintree = load_kintree_kinect(pk_root)\n",
    "\n",
    "            #infer SMPLX*\n",
    "            body = exp_bm(rot_t, pose_body_t, pose_hand_t, pose_jaw_t, pose_eye_t, betas_t, trans_t, expr_t)\n",
    "            #infer SMPLX\n",
    "            verts = body.v[0].detach().cpu().numpy()\n",
    "\n",
    "            #get predicted 3d vertices\n",
    "            joints_pred_3d = verts[-32:].reshape(-1, 3)\n",
    "\n",
    "            #render the SMPLX model\n",
    "            main_renderer.render(verts, np.eye(3), K=K_rgb)\n",
    "            cimg = main_renderer.get_aligned_camera_image()[:, :, :3]\n",
    "            \n",
    "            cimgs.append(cimg)\n",
    "\n",
    "        # crop face\n",
    "        P_cw = K_rgb @ T_depth_to_rgb[0:3, 0:4]\n",
    "        keypoints_2d = project_kinect_joints(joints_poses, P_cw, im_size)\n",
    "\n",
    "        face_bbox = crop_bbox_by_keypoints_2d(keypoints_2d[KINECT_FACE_INDICES])\n",
    "\n",
    "        img_croppped = utils.common.crop_image(img, face_bbox)\n",
    "        cimgs_cropped = [utils.common.crop_image(cimg, face_bbox) for cimg in cimgs]\n",
    "\n",
    "        \n",
    "        cobmibned_img = np.concatenate([img_croppped, cimgs_cropped[0], cimgs_cropped[1]], axis=1)\n",
    "        if count % 50 == 0:\n",
    "            plt.imshow(cobmibned_img)\n",
    "            plt.show()\n",
    "        \n",
    "        img_path = os.path.join(frame_dir, f\"{count:06d}.jpg\")\n",
    "        cv2.imwrite(img_path, cobmibned_img[:, :, ::-1])\n",
    "        \n",
    "        count += 1\n",
    "#     except Exception as e:\n",
    "#         print(f\"(!) Failed for reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    \"ffmpeg\",\n",
    "    \"-y\",\n",
    "    \"-framerate\", \"25\",\n",
    "    \"-i\", os.path.join(frame_dir, \"%06d.jpg\"),\n",
    "    \"-c:v\", \"libx264\",\n",
    "    \"-vf\", \"fps=25\",\n",
    "    \"-vf\", \"pad=ceil(iw/2)*2:ceil(ih/2)*2\",\n",
    "    \"-pix_fmt\", \"yuv420p\",\n",
    "    output_video_path\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "if result.returncode:\n",
    "    raise ValueError(result.stderr.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Output video path:\\n{output_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
